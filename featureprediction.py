# -*- coding: utf-8 -*-
"""FeaturePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17cCgULVPuzryi-EK_OrGDTpBy9UBM6F_
"""

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
!pip install uniprot
!pip install biopython
import uniprot
from Bio.SeqUtils.ProtParam import ProteinAnalysis

from google.colab import drive
drive.mount('/content/drive')

seqids, fastas = uniprot.read_fasta('/content/drive/MyDrive/uniprot_sprot.fasta')

seqids[0:10]

fastas['sp|P09430']

analysis = ProteinAnalysis(fastas[seqids[0]]['sequence'])

analysis.molecular_weight()

#Protein analysis function for obtaining measures, and .
from sklearn.naive_bayes import GaussianNB
naive_model=GaussianNB()
split_variable=0


#Instability
#Dictionary with [0] unstable protein seqs. and [1] stable protien seqs.
stability_data_groups={}
split_variable=40

for seqid in seqids:
  for fasta in fastas:
    instability_data=ProteinAnalysis(fastas[seqid]['sequence'])
    #stability_data[seqid]=instability_data.instability_index
    if instability_data.instability_index()>split_variable:
      stability_data_groups[0]=stability_data_groups[0]+ fasta
    else:
      stability_data_groups[1]=stability_data_groups[1]+ fasta

  #Now we split into training/testing data and implement a naive bayes model,
  #but I need vector data to fit the model
naive_model.fit(unstable_vectors, stable_vectors)
naive_model.predict(test_vectors)
print(naive_model.score(test_vectors))


#Molecular Weight
split_variable=0.5

#TODO: Dictionary with what groups? also change the splitvariable^
molecWeight_data_groups={}
for seqid, fasta in seqids, fastas:
  #analyze protein
  pro_analysis_data=ProteinAnalysis(fastas[seqid]['sequence'])
  #we need the molecular weight
  #weight_data[seqid]=pro_analysis_data.molecular_weight()
  weight_data=pro_analysis_data.molecular_weight()

  if weight_data>split_variable:
      new_dict={}
      new_dict=molecWeight_data_groups[0]
      molecWeight_data_groups[0]=new_dict+ fasta
  else:
      molecWeight_data_groups[1]=molecWeight_data_groups[1]+ fasta

model, alphabet = torch.hub.load("facebookresearch/esm:main", "esm1b_t33_650M_UR50S")

batch_converter = alphabet.get_batch_converter()
model.eval()  # disables dropout for deterministic results

# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)
data = [
    ("protein1", "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG"),
    ("protein2", "KALTARQQEVFDLIRDHISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE"),
    ("protein2 with mask","KALTARQQEVFDLIRD<mask>ISQTGMPPTRAEIAQRLGFRSPNAAEEHLKALARKGVIEIVSGASRGIRLLQEE"),
    ("protein3",  "K A <mask> I S Q"),
]
batch_labels, batch_strs, batch_tokens = batch_converter(data)

# Extract per-residue representations (on CPU)
with torch.no_grad():
    results = model(batch_tokens, repr_layers=[33], return_contacts=True)
token_representations = results["representations"][33]

# Generate per-sequence representations via averaging
# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.
sequence_representations = []
for i, (_, seq) in enumerate(data):
    sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))

# Look at the unsupervised self-attention map contact predictions
import matplotlib.pyplot as plt
for (_, seq), attention_contacts in zip(data, results["contacts"]):
    plt.matshow(attention_contacts[: len(seq), : len(seq)])
    plt.title(seq)
    plt.show()

sequence_representations[3].shape

token_representations[3].shape